{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import random\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.lines import Line2D\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.model_selection import train_test_split, TimeSeriesSplit\n",
    "from sklearn.linear_model import LinearRegression, HuberRegressor\n",
    "from sklearn.metrics import mean_squared_error, r2_score\n",
    "import xgboost as xgb\n",
    "from glob import glob\n",
    "from tqdm import tqdm\n",
    "import tensorflow as tf\n",
    "\n",
    "tf.logging.set_verbosity(tf.logging.ERROR)\n",
    "os.environ['TF_CPP_MIN_LOG_LEVEL'] = '3'\n",
    "\n",
    "from keras.layers import Input, Dense, LSTM, BatchNormalization, Flatten\n",
    "from keras.models import Model\n",
    "from keras.initializers import glorot_uniform\n",
    "from keras.preprocessing.sequence import TimeseriesGenerator\n",
    "from keras import optimizers, regularizers, activations\n",
    "from keras.callbacks import ModelCheckpoint, EarlyStopping, ReduceLROnPlateau, TensorBoard\n",
    "import keras.backend as k"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define a number of deep models as functions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### LSTM model\n",
    "- 2 LSTM layers\n",
    "- Dense layer\n",
    "- Batch Normalization layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def lstm1(input_shape_0, input_shape_1, activation, seed):\n",
    "    input_tensor = Input(shape=(input_shape_0, input_shape_1),\n",
    "                         dtype='float32',\n",
    "                         name='input_tensor')\n",
    "\n",
    "    lstm_1 = LSTM(units=32,\n",
    "                  activation=activation,\n",
    "                  dropout=0.2,\n",
    "                  recurrent_dropout=0.2,\n",
    "                  return_sequences=True,\n",
    "                  kernel_initializer=glorot_uniform(random.seed(seed)),\n",
    "                  name='lstm_1')(input_tensor)\n",
    "\n",
    "    lstm_2 = LSTM(units=32,\n",
    "                  activation=activation,\n",
    "                  dropout=0.2,\n",
    "                  recurrent_dropout=0.2,\n",
    "                  return_sequences=False,\n",
    "                  kernel_initializer=glorot_uniform(random.seed(seed)),\n",
    "                  name='lstm_2')(lstm_1)\n",
    "\n",
    "    dense = Dense(units=32,\n",
    "                  activation=activation,\n",
    "                  kernel_regularizer=l1(0.001),\n",
    "                  kernel_initializer=glorot_uniform(random.seed(seed)),\n",
    "                  name='dense')(lstm_2)\n",
    "\n",
    "    batch_norm = BatchNormalization(axis=-1)(dense)\n",
    "\n",
    "    # add a regression layer\n",
    "    output_tensor = Dense(units=1,\n",
    "                          activation=None,\n",
    "                          name='output_tensor')(batch_norm)\n",
    "\n",
    "    # specify input and output\n",
    "    return Model(input_tensor, output_tensor)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### NN model with 1 Dense layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def nn1(input_shape_0, input_shape_1, activation, seed, kernel_regularizer):\n",
    "    input_tensor = Input(shape=(input_shape_0, input_shape_1),\n",
    "                         dtype='float32',\n",
    "                         name='input_tensor')\n",
    "\n",
    "    dense_1 = Dense(units=32,\n",
    "                    activation=activation,\n",
    "                    kernel_regularizer=kernel_regularizer,\n",
    "                    kernel_initializer=glorot_uniform(random.seed(seed)),\n",
    "                    name='dense_1')(input_tensor)\n",
    "\n",
    "    dense_1_flatten = Flatten(name='dense_1_flattened')(dense_1)\n",
    "\n",
    "    # add a regression layer\n",
    "    output_tensor = Dense(units=1,\n",
    "                          activation=None,\n",
    "                          name='output_tensor')(dense_1_flatten)\n",
    "\n",
    "    # specify input and output\n",
    "    return Model(input_tensor, output_tensor)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### NN model with 2 Dense layers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def nn2(input_shape_0, input_shape_1, activation, seed, kernel_regularizer):\n",
    "    input_tensor = Input(shape=(input_shape_0, input_shape_1),\n",
    "                         dtype='float32',\n",
    "                         name='input_tensor')\n",
    "\n",
    "    dense_1 = Dense(units=32,\n",
    "                    activation=activation,\n",
    "                    kernel_regularizer=kernel_regularizer,\n",
    "                    kernel_initializer=glorot_uniform(random.seed(seed)),\n",
    "                    name='dense_1')(input_tensor)\n",
    "\n",
    "    dense_2 = Dense(units=16,\n",
    "                    activation=activation,\n",
    "                    kernel_regularizer=kernel_regularizer,\n",
    "                    kernel_initializer=glorot_uniform(random.seed(seed)),\n",
    "                    name='dense_2')(dense_1)\n",
    "\n",
    "    dense_2_flatten = Flatten(name='dense_2_flatten')(dense_2)\n",
    "\n",
    "    # add a regression layer\n",
    "    output_tensor = Dense(units=1,\n",
    "                          activation=None,\n",
    "                          name='output_tensor')(dense_2_flatten)\n",
    "\n",
    "    # specify input and output\n",
    "    return Model(input_tensor, output_tensor)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### NN model with 3 Dense layers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def nn3(input_shape_0, input_shape_1, activation, seed, kernel_regularizer):\n",
    "    input_tensor = Input(shape=(input_shape_0, input_shape_1),\n",
    "                         dtype='float32',\n",
    "                         name='input_tensor')\n",
    "\n",
    "    dense_1 = Dense(units=32,\n",
    "                    activation=activation,\n",
    "                    kernel_regularizer=kernel_regularizer,\n",
    "                    kernel_initializer=glorot_uniform(random.seed(seed)),\n",
    "                    name='dense_1')(input_tensor)\n",
    "\n",
    "    dense_2 = Dense(units=16,\n",
    "                    activation=activation,\n",
    "                    kernel_regularizer=kernel_regularizer,\n",
    "                    kernel_initializer=glorot_uniform(random.seed(seed)),\n",
    "                    name='dense_2')(dense_1)\n",
    "\n",
    "    dense_3 = Dense(units=8,\n",
    "                    activation=activation,\n",
    "                    kernel_regularizer=kernel_regularizer,\n",
    "                    kernel_initializer=glorot_uniform(random.seed(seed)),\n",
    "                    name='dense_3')(dense_2)\n",
    "\n",
    "    dense_3_flatten = Flatten(name='dense_2_flatten')(dense_3)\n",
    "\n",
    "    # add a regression layer\n",
    "    output_tensor = Dense(units=1,\n",
    "                          activation=None,\n",
    "                          name='output_tensor')(dense_3_flatten)\n",
    "\n",
    "    # specify input and output\n",
    "    return Model(input_tensor, output_tensor)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### NN model with 4 Dense layers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def nn4(input_shape_0, input_shape_1, activation, seed, kernel_regularizer):\n",
    "    input_tensor = Input(shape=(input_shape_0, input_shape_1),\n",
    "                         dtype='float32',\n",
    "                         name='input_tensor')\n",
    "\n",
    "    dense_1 = Dense(units=32,\n",
    "                    activation=activation,\n",
    "                    kernel_regularizer=kernel_regularizer,\n",
    "                    kernel_initializer=glorot_uniform(random.seed(seed)),\n",
    "                    name='dense_1')(input_tensor)\n",
    "\n",
    "    dense_2 = Dense(units=16,\n",
    "                    activation=activation,\n",
    "                    kernel_regularizer=kernel_regularizer,\n",
    "                    kernel_initializer=glorot_uniform(random.seed(seed)),\n",
    "                    name='dense_2')(dense_1)\n",
    "\n",
    "    dense_3 = Dense(units=8,\n",
    "                    activation=activation,\n",
    "                    kernel_regularizer=kernel_regularizer,\n",
    "                    kernel_initializer=glorot_uniform(random.seed(seed)),\n",
    "                    name='dense_3')(dense_2)\n",
    "\n",
    "    dense_4 = Dense(units=4,\n",
    "                    activation=activation,\n",
    "                    kernel_regularizer=kernel_regularizer,\n",
    "                    kernel_initializer=glorot_uniform(random.seed(seed)),\n",
    "                    name='dense_4')(dense_3)\n",
    "\n",
    "    dense_4_flatten = Flatten(name='dense_4_flatten')(dense_4)\n",
    "\n",
    "    # add a regression layer\n",
    "    output_tensor = Dense(units=1,\n",
    "                          activation=None,\n",
    "                          name='output_tensor')(dense_4_flatten)\n",
    "\n",
    "    # specify input and output\n",
    "    return Model(input_tensor, output_tensor)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### NN model with 5 Dense layers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def nn5(input_shape_0, input_shape_1, activation, seed, kernel_regularizer):\n",
    "    input_tensor = Input(shape=(input_shape_0, input_shape_1),\n",
    "                         dtype='float32',\n",
    "                         name='input_tensor')\n",
    "\n",
    "    dense_1 = Dense(units=32,\n",
    "                    activation=activation,\n",
    "                    kernel_regularizer=kernel_regularizer,\n",
    "                    kernel_initializer=glorot_uniform(random.seed(seed)),\n",
    "                    name='dense_1')(input_tensor)\n",
    "\n",
    "    dense_2 = Dense(units=16,\n",
    "                    activation=activation,\n",
    "                    kernel_regularizer=kernel_regularizer,\n",
    "                    kernel_initializer=glorot_uniform(random.seed(seed)),\n",
    "                    name='dense_2')(dense_1)\n",
    "\n",
    "    dense_3 = Dense(units=8,\n",
    "                    activation=activation,\n",
    "                    kernel_regularizer=kernel_regularizer,\n",
    "                    kernel_initializer=glorot_uniform(random.seed(seed)),\n",
    "                    name='dense_3')(dense_2)\n",
    "\n",
    "    dense_4 = Dense(units=4,\n",
    "                    activation=activation,\n",
    "                    kernel_regularizer=kernel_regularizer,\n",
    "                    kernel_initializer=glorot_uniform(random.seed(seed)),\n",
    "                    name='dense_4')(dense_3)\n",
    "\n",
    "    dense_5 = Dense(units=4,\n",
    "                    activation=activation,\n",
    "                    kernel_regularizer=kernel_regularizer,\n",
    "                    kernel_initializer=glorot_uniform(random.seed(seed)),\n",
    "                    name='dense_5')(dense_4)\n",
    "\n",
    "    dense_5_flatten = Flatten(name='dense_5_flatten')(dense_5)\n",
    "\n",
    "    # add a regression layer\n",
    "    output_tensor = Dense(units=1,\n",
    "                          activation=None,\n",
    "                          name='output_tensor')(dense_5_flatten)\n",
    "\n",
    "    # specify input and output\n",
    "    return Model(input_tensor, output_tensor)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define a class for Neural Network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class StocksNN:\n",
    "    def __init__(self, features_file_path, period, lookback, step, features, target):\n",
    "        \"\"\"\n",
    "        :param features_file_path: path to the csv file that includes the features\n",
    "        :param period: daily or weekly\n",
    "        :param lookback: number of time-steps in the past to use for predicting the next time-step\n",
    "        :param step: number of steps for sampling\n",
    "        :param features: a list of the features to use in the model\n",
    "        :param target: return or excess_return\n",
    "        \"\"\"\n",
    "\n",
    "        self.period = period\n",
    "        self.lookback = lookback\n",
    "        self.step = step\n",
    "        self.target = target\n",
    "\n",
    "        # make sure there's a sub directory to save results to\n",
    "        self.save_in = os.path.join(os.getcwd(), 'results', f'{self.period}_features')\n",
    "        if not os.path.exists(self.save_in):\n",
    "            os.makedirs(self.save_in)\n",
    "\n",
    "        # concatenate a unique string for this model\n",
    "        file_name = os.path.basename(features_file_path)\n",
    "        file_root, file_ext = os.path.splitext(file_name)\n",
    "        self.unique_string = '_'.join(file_root.lower().split())\n",
    "\n",
    "        # ------------------------------------------------- READ DATA\n",
    "        # read data and choose only features wanted for this model from the multi-indexed header\n",
    "        self.df = pd.read_csv(features_file_path, index_col=[0], header=[0, 1], parse_dates=True)\n",
    "        self.df = self.df[features].droplevel(level=0, axis=1)\n",
    "\n",
    "        # drop either return or excess_return\n",
    "        if self.target == 'return':\n",
    "            self.df = self.df.drop('excess_return', axis=1)\n",
    "        elif self.target == 'excess_return':\n",
    "            self.df = self.df.drop('return', axis=1)        \n",
    "\n",
    "        # ------------------------------------------------- SELECT DATA INDICES FOR SPLITTING\n",
    "        # split data 80/20 for training/testing without shuffling to keep time order\n",
    "        # keep target as a feature in data\n",
    "        # the initial training data will be split later into n splits\n",
    "        self.X_train_init, self.X_test_init = train_test_split(self.df, test_size=0.20, shuffle=False)        \n",
    "\n",
    "        # select a number of splits equals to the number of years in training data\n",
    "        self.n_split = len(set(self.X_train_init.index.year))\n",
    "\n",
    "        # ---------------------------------------------------- DATA SCALING\n",
    "        # fit scaler to training data only (including target as it will be a feature as well)\n",
    "        # select target\n",
    "        self.scaler = StandardScaler()\n",
    "        self.X_train_init = self.scaler.fit_transform(self.X_train_init)\n",
    "        self.y_train_init = self.X_train_init[:, -1]\n",
    "\n",
    "        # transform test data (including target as it will be a feature as well)\n",
    "        # select target\n",
    "        self.X_test = self.scaler.transform(self.X_test_init)\n",
    "        self.y_test = self.X_test[:, -1]\n",
    "\n",
    "    # ------------------------------------------------- BUILD & COMPILE MODEL\n",
    "    def build_and_compile_model(self, model_function, model_name, activation, optimizer, seed, kernel_regularizer):\n",
    "        self.input_shape_0 = self.lookback // self.step\n",
    "        self.input_shape_1 = self.X_train_init.shape[1]\n",
    "\n",
    "        self.model = model_function(self.input_shape_0, self.input_shape_1, activation, seed, kernel_regularizer)\n",
    "\n",
    "        self.model.compile(optimizer=optimizer,\n",
    "                           loss='mse')\n",
    "        \n",
    "        # append unique string\n",
    "        self.unique_string = f'{self.unique_string}_{model_name}_{round(k.eval(self.model.optimizer.lr), 3)}'\n",
    "\n",
    "    # ------------------------------------------------- WALK FORWARD VALIDATION\n",
    "    def walk_forward_validation(self, batch_size, epochs, n_splits=None):\n",
    "\n",
    "        self.batch_size = batch_size\n",
    "\n",
    "        # if n_splits is not provided, use number of years in training data as above\n",
    "        if n_splits is None:\n",
    "            n_splits = self.n_split\n",
    "\n",
    "        # split initial training data for training/validation\n",
    "        # forward walking validation\n",
    "        tscv = TimeSeriesSplit(n_splits=n_splits)\n",
    "        eval_scores = list()\n",
    "        for train_index, val_index in tscv.split(self.X_train_init):\n",
    "            X_train, X_val = self.X_train_init[train_index], self.X_train_init[val_index]\n",
    "            y_train, y_val = self.y_train_init[train_index], self.y_train_init[val_index]            \n",
    "\n",
    "            # ----------------------------------------- CREATE GENERATORS\n",
    "            # generator output is a list of batches\n",
    "            # each batch is a tuple of (samples, targets)\n",
    "            train_gen = TimeseriesGenerator(X_train,\n",
    "                                            y_train,\n",
    "                                            length=self.lookback,\n",
    "                                            sampling_rate=self.step,\n",
    "                                            stride=1,\n",
    "                                            batch_size=self.batch_size)            \n",
    "\n",
    "            val_gen = TimeseriesGenerator(X_val,\n",
    "                                          y_val,\n",
    "                                          length=self.lookback,\n",
    "                                          sampling_rate=self.step,\n",
    "                                          stride=1,\n",
    "                                          batch_size=self.batch_size)\n",
    "            \n",
    "            # ----------------------------------------- FIT & EVALUATE            \n",
    "            # interrupt training when improvement stops\n",
    "            # continually save the model during training (only current best)\n",
    "            # reduce learning rate when the validation loss has stopped improving\n",
    "            callbacks_list = [EarlyStopping(monitor='val_loss',\n",
    "                                            patience=5),\n",
    "                              ModelCheckpoint(filepath=os.path.join(self.save_in, f'{self.unique_string}.h5'),\n",
    "                                              monitor='val_loss',\n",
    "                                              save_best_only=True),\n",
    "                              ReduceLROnPlateau(monitor='val_loss',\n",
    "                                                factor=0.1,\n",
    "                                                patience=10)\n",
    "                             ]\n",
    "\n",
    "            self.model.fit_generator(train_gen,\n",
    "                                     steps_per_epoch=len(train_gen),\n",
    "                                     epochs=epochs,\n",
    "                                     validation_data=val_gen,\n",
    "                                     validation_steps=len(val_gen),\n",
    "                                     callbacks=callbacks_list,\n",
    "                                     verbose=False)\n",
    "\n",
    "            # get eval metric and append to list\n",
    "            eval_score = self.model.evaluate_generator(val_gen, steps=len(val_gen))\n",
    "            eval_scores.append(eval_score)\n",
    "\n",
    "        # average evaluation scores\n",
    "        return pd.Series(np.mean(eval_scores), index=['mse'])\n",
    "\n",
    "    def predict_on_test_data_nn(self):\n",
    "        self.test_gen = TimeseriesGenerator(self.X_test,\n",
    "                                            self.y_test,\n",
    "                                            length=self.lookback,\n",
    "                                            sampling_rate=self.step,\n",
    "                                            stride=1,\n",
    "                                            batch_size=self.batch_size)        \n",
    "\n",
    "        self.predictions = self.model.predict_generator(self.test_gen,\n",
    "                                                        steps=len(self.test_gen))\n",
    "\n",
    "        # inverse transform predictions to un-normalize\n",
    "        # first, repeat as many times as the transformer expects\n",
    "        self.predictions = np.repeat(self.predictions, self.df.shape[1], axis=1)\n",
    "        self.predictions = self.scaler.inverse_transform(self.predictions)[:, -1]\n",
    "\n",
    "        # get test data using the generator to get corresponding targets\n",
    "        # each batch is a tuple of numpy arrays (samples, targets), get the targets as a list comprehension\n",
    "        # convert list of numpy arrays into a numpy vector, reshape into 1D array\n",
    "        self.true = np.concatenate([batch[1] for batch in self.test_gen], axis=0).reshape(-1, 1)\n",
    "\n",
    "        # inverse transform targets to un-normalize\n",
    "        # first, repeat as many times as the transformer expects\n",
    "        self.true = np.repeat(self.true, self.df.shape[1], axis=1)\n",
    "        self.true = self.scaler.inverse_transform(self.true)[:, -1]\n",
    "\n",
    "        # index was lost when scaling. Get the index from self.X_test_init\n",
    "        # however, we will loose few of the last samples in self.test_gen.\n",
    "        # use the length of self.true to get the exact number of test samples used\n",
    "        return pd.DataFrame({'true': self.true, 'predictions': self.predictions},\n",
    "                            index=self.X_test_init.index[:self.true.shape[0]])\n",
    "\n",
    "    def predict_on_test_data_lewellen(self, n_splits=None):\n",
    "        \n",
    "        df_size_factors = None\n",
    "        \n",
    "        # if n_splits is not provided, use number of years in training data as above\n",
    "        if n_splits is None:\n",
    "            n_splits = self.n_split\n",
    "        \n",
    "        if self.period == 'daily':\n",
    "            df_size_factors = pd.read_csv(os.path.join(os.getcwd(), 'data', 'Daily_F_F_Research_Data_Factors.csv'),\n",
    "                                          index_col=0,\n",
    "                                          parse_dates=True)\n",
    "        elif self.period == 'weekly':\n",
    "            df_size_factors = pd.read_csv(os.path.join(os.getcwd(), 'data', 'Weekly_F_F_Research_Data_Factors.csv'),\n",
    "                                          index_col=0,\n",
    "                                          parse_dates=True).resample('W').last()\n",
    "\n",
    "        # select only indexes in stocks\n",
    "        df_size_factors = df_size_factors[df_size_factors.index.isin(self.df.index)]\n",
    "\n",
    "        # select target and shift by 1 time-step and drop first row\n",
    "        # concatenate features and remove first row\n",
    "        y = self.df[self.target].shift(self.lookback).dropna()\n",
    "        X = pd.concat([df_size_factors['SMB'], self.df['df_book_to_market'], self.df['momentum_1M']],\n",
    "                      axis=1, join='inner').iloc[self.lookback:]\n",
    "\n",
    "        # rows should have the same order as the first train_test_split\n",
    "        X_train, X_test, y_train, true_lewellen = train_test_split(X, y, test_size=0.20, shuffle=False)\n",
    "        \n",
    "        # fit scaler to training data only\n",
    "        # transform test data\n",
    "        scaler = StandardScaler()\n",
    "        X_train = scaler.fit_transform(X_train)\n",
    "        X_test = scaler.transform(X_test)\n",
    "        \n",
    "        # create a Huber regressor        \n",
    "        model = HuberRegressor(alpha=0.0001)\n",
    "\n",
    "        # split initial training data for training/validation\n",
    "        # forward walking validation\n",
    "        tscv = TimeSeriesSplit(n_splits=n_splits)\n",
    "        eval_scores = list()\n",
    "        for train_index, val_index in tscv.split(self.X_train_init):\n",
    "            X_train, X_val = self.X_train_init[train_index], self.X_train_init[val_index]\n",
    "            y_train, y_val = self.y_train_init[train_index], self.y_train_init[val_index]            \n",
    "\n",
    "            # fit Huber regressor        \n",
    "            model = HuberRegressor(alpha=0.0001)\n",
    "            model.fit(X_train, y_train)\n",
    "\n",
    "        # predict on test data\n",
    "        predictions_lewellen = model.predict(X_test)\n",
    "\n",
    "        # create df with all time-steps. Select only indexes that are in the self.X_test_init\n",
    "        df = pd.DataFrame({'true': true_lewellen, 'predictions': predictions_lewellen})\n",
    "        \n",
    "        return df[df.index.isin(self.X_test_init.index[:self.true.shape[0]])]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define a class for XGBoost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class StocksXGB:\n",
    "    def __init__(self, features_file_path, period, lookback, step, features, target):\n",
    "        \"\"\"\n",
    "        :param features_file_path: path to the csv file that includes the features\n",
    "        :param period: daily or weekly\n",
    "        :param lookback: number of time-steps in the past to use for predicting the next time-step\n",
    "        :param step: number of steps for sampling\n",
    "        :param features: a list of the features to use in the model\n",
    "        :param target: return or excess_return\n",
    "        \"\"\"\n",
    "        \n",
    "        self.period = period\n",
    "        self.lookback = lookback\n",
    "        self.step = step\n",
    "        self.target = target\n",
    "\n",
    "        # make sure there's a sub directory to save results to\n",
    "        self.save_in = os.path.join(os.getcwd(), 'results', f'{self.period}_features')\n",
    "        if not os.path.exists(self.save_in):\n",
    "            os.makedirs(self.save_in)\n",
    "\n",
    "        # concatenate a unique string for this model\n",
    "        file_name = os.path.basename(features_file_path)\n",
    "        file_root, file_ext = os.path.splitext(file_name)\n",
    "        self.unique_string = '_'.join(file_root.lower().split())\n",
    "\n",
    "        # ------------------------------------------------- READ DATA\n",
    "        # read data and choose only features wanted for this model from the multi-indexed header\n",
    "        self.df = pd.read_csv(features_file_path, index_col=[0], header=[0, 1], parse_dates=True)\n",
    "        self.df = self.df[features].droplevel(level=0, axis=1)\n",
    "\n",
    "        # drop either return or excess_return\n",
    "        if self.target == 'return':\n",
    "            self.df = self.df.drop('excess_return', axis=1)\n",
    "        elif self.target == 'excess_return':\n",
    "            self.df = self.df.drop('return', axis=1)\n",
    "\n",
    "        # ------------------------------------------------- FEATURES EXTRACTIONS\n",
    "        # extract features to infer time\n",
    "        # this's important in xgb as it selects samples randomly so time-order is lost\n",
    "        self.df['month'] = self.df.index.month\n",
    "        self.df['quarter'] = self.df.index.quarter\n",
    "        self.df['year'] = self.df.index.year\n",
    "\n",
    "        if self.period == 'daily':\n",
    "            self.df['dayofyear'] = self.df.index.dayofyear\n",
    "            self.df['dayofmonth'] = self.df.index.day\n",
    "        elif self.period == 'weekly':\n",
    "            self.df['weekofyear'] = self.df.index.weekofyear\n",
    "\n",
    "        # shift return by lookback so the target is not the current return but in the future\n",
    "        # this is similar to what the keras generator does for the NN model\n",
    "        # the old return will be used as a normal feature now (similar to the NN model as well)\n",
    "        self.df['return_future'] = self.df[self.target].shift(self.lookback)\n",
    "\n",
    "        # ------------------------------------------------- SPLIT DATA\n",
    "        # split data 80/20 for training/testing without shuffling to keep time order\n",
    "        # keep target as a feature in data\n",
    "        # the initial training data will be split later into n splits\n",
    "        self.X_train_init, self.X_test_init = train_test_split(self.df.dropna(), test_size=0.20, shuffle=False)\n",
    "\n",
    "        # select a number of splits equals to the number of years in training data\n",
    "        self.n_split = len(set(self.X_train_init.index.year))\n",
    "\n",
    "        # trees don't require scaling or centering of data\n",
    "        self.y_train_init = self.X_train_init['return_future'].values\n",
    "        self.X_train_init = self.X_train_init.drop(['return_future'], axis=1).values\n",
    "\n",
    "        self.y_test = self.X_test_init['return_future'].values\n",
    "        self.X_test = self.X_test_init.drop(['return_future'], axis=1).values\n",
    "\n",
    "    # ------------------------------------------------- BUILD & COMPILE MODEL\n",
    "    def build_model(self, objective, max_depth, learning_rate, n_estimators, seed):\n",
    "        self.max_depth = max_depth\n",
    "        self.learning_rate = learning_rate\n",
    "        self.n_estimators = n_estimators\n",
    "\n",
    "        self.xgb_model = xgb.XGBRegressor(objective=objective,\n",
    "                                          max_depth=self.max_depth,\n",
    "                                          learning_rate=self.learning_rate,\n",
    "                                          n_estimators=self.n_estimators,\n",
    "                                          random_state=seed)\n",
    "\n",
    "        # append unique string\n",
    "        self.unique_string = f\"{self.unique_string}_xgb_{self.max_depth}_{round(self.learning_rate, 3)}_{self.n_estimators}\"\n",
    "\n",
    "    # ------------------------------------------------- WALK FORWARD VALIDATION\n",
    "    def walk_forward_validation(self, n_splits=None):\n",
    "\n",
    "        # if n_splits is not provided, use number of years in training data as above\n",
    "        if n_splits is None:\n",
    "            n_splits = self.n_split\n",
    "\n",
    "        # split initial training data for training/validation\n",
    "        # forward walking validation\n",
    "        tscv = TimeSeriesSplit(n_splits=n_splits)\n",
    "        eval_scores = list()\n",
    "        for train_index, val_index in tscv.split(self.X_train_init):\n",
    "            X_train, X_val = self.X_train_init[train_index], self.X_train_init[val_index]\n",
    "            y_train, y_val = self.y_train_init[train_index], self.y_train_init[val_index]\n",
    "            # print(f'X_train: {len(X_train)}', f'X_val: {len(X_val)}')\n",
    "\n",
    "            # ----------------------------------------- FIT & EVALUATE\n",
    "            self.xgb_model.fit(X_train,\n",
    "                               y_train.ravel(),\n",
    "                               eval_set=[(X_val, y_val)],\n",
    "                               eval_metric=['rmse'],\n",
    "                               verbose=False)\n",
    "\n",
    "            # get the last tree eval metrics and append to list\n",
    "            # square rmse to get mse\n",
    "            eval_score = self.xgb_model.evals_result_['validation_0']['rmse'][-1] ** 2\n",
    "            eval_scores.append(eval_score)\n",
    "\n",
    "            # average evaluation scores\n",
    "            return pd.Series(np.mean(eval_scores), index=['mse'])\n",
    "\n",
    "    def predict_on_test_data(self):\n",
    "        predictions = self.xgb_model.predict(self.X_test)\n",
    "\n",
    "        # index was lost when scaling. Get the index from self.X_test_init\n",
    "        return pd.DataFrame({'true': self.y_test, 'predictions': predictions}, index=self.X_test_init.index)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test Results: scores and plots"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Calculate predicted vs true scores for test data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calc_predictions_avg_test_scores(true, predictions):\n",
    "    \"\"\"\n",
    "    Calculate test scores between true and predicted returns\n",
    "    :param true: true returns\n",
    "    :param predictions: predicted returns\n",
    "    :return: Pandas Series\n",
    "    \"\"\"\n",
    "    r2_score_modified = 1 - (np.sum((true - predictions) ** 2)) / (np.sum(true ** 2))\n",
    "    r2_score_orig = r2_score(true, predictions)\n",
    "    mse = mean_squared_error(true, predictions)\n",
    "    sse = np.sum((predictions - true) ** 2)\n",
    "\n",
    "    return pd.Series([r2_score_modified, r2_score_orig, mse, sse],\n",
    "                     index=['r2_modified', 'r2_original', 'mse', 'sse'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Plot predicted vs true returns for test data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_predictions_avg_stock(df_models_avg_true_best, df_models_avg_pre_best, df_benchmarks, save_in, unique_string):\n",
    "    \"\"\"\n",
    "    Plot the averaged predicted returns vs true for model, and lewellen per stock\n",
    "    :param df_predictions_avg: Pandas DataFrame\n",
    "    :param save_in: directory to save figure\n",
    "    :param unique_string: to add to the figure name\n",
    "    :return:\n",
    "    \"\"\"\n",
    "    start_time = df_models_avg_true_best.index[0]\n",
    "    end_time = df_models_avg_true_best.index[-1]\n",
    "    stocks = set(df_models_avg_true_best.columns.get_level_values(1))\n",
    "\n",
    "    custom_lines = [Line2D([0], [0], color='b', lw=2),\n",
    "                    Line2D([0], [0], color='r', lw=2)]\n",
    "\n",
    "    # loop over unique stocks found in the second level\n",
    "    for stock in stocks:\n",
    "        fig, ax = plt.subplots(2, 2, figsize=(12, 6))\n",
    "        fig.suptitle(f'{stock} avg. Predictions', fontsize=12)\n",
    "\n",
    "        ax[0, 0].plot(df_models_avg_true_best['NN', stock], 'b', label='Actual')\n",
    "        ax[0, 0].plot(df_models_avg_pre_best['NN', stock], 'r', label='Predicted')\n",
    "        ax[0, 0].set_xticks([ax[0, 0].get_xticks()[0], ax[0, 0].get_xticks()[-1]], minor=False)\n",
    "        ax[0, 0].set_xticklabels([start_time.strftime(\"%b %Y\"), end_time.strftime(\"%b %Y\")])\n",
    "        ax[0, 0].set_title(f\"NN, lr={df_models_avg_true_best['NN', stock].columns.values}\", fontsize=10)\n",
    "\n",
    "        ax[0, 1].plot(df_models_avg_true_best['XGB', stock], 'b', label='Actual')\n",
    "        ax[0, 1].plot(df_models_avg_pre_best['XGB', stock], 'r', label='Predicted')\n",
    "        ax[0, 1].set_xticks([ax[0, 1].get_xticks()[0], ax[0, 1].get_xticks()[-1]], minor=False)\n",
    "        ax[0, 1].set_xticklabels([start_time.strftime(\"%b %Y\"), end_time.strftime(\"%b %Y\")])\n",
    "        ax[0, 1].set_title(f\"XGBoost, lr={df_models_avg_true_best['XGB', stock].columns.values}\", fontsize=10)\n",
    "\n",
    "        ax[1, 0].plot(df_benchmarks['lewellen', stock, 'true'], 'b', label='Actual')\n",
    "        ax[1, 0].plot(df_benchmarks['lewellen', stock, 'predictions'], 'r', label='Predicted')\n",
    "        ax[1, 0].set_xticks([ax[1, 0].get_xticks()[0], ax[1, 0].get_xticks()[-1]], minor=False)\n",
    "        ax[1, 0].set_xticklabels([start_time.strftime(\"%b %Y\"), end_time.strftime(\"%b %Y\")])\n",
    "        ax[1, 0].set_title('Lewellen', fontsize=10)\n",
    "\n",
    "        ax[1, 1].legend(custom_lines, ['Actual', 'Predicted'], loc='center')\n",
    "        ax[1, 1].axis('off')\n",
    "\n",
    "        plt.savefig(os.path.join(save_in, f'{unique_string}_predictions_avg_{stock}.png'),\n",
    "                    orientation='portrait',\n",
    "                    format='png')\n",
    "        plt.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_predictions_avg_all_stocks(df_models_avg_true_best, df_models_avg_pre_best, df_benchmarks, save_in, unique_string):\n",
    "    \"\"\"\n",
    "    Plot the averaged predicted returns vs true for model, and lewellen for all stocks\n",
    "    :param df_predictions_avg: Pandas DataFrame\n",
    "    :param save_in: directory to save figure\n",
    "    :param unique_string: to add to the figure name\n",
    "    :return:\n",
    "    \"\"\"\n",
    "    start_time = df_models_avg_true_best.index[0]\n",
    "    end_time = df_models_avg_true_best.index[-1]\n",
    "    stocks = set(df_models_avg_true_best.columns.get_level_values(1))\n",
    "\n",
    "    fig, ax = plt.subplots(2, 2, figsize=(12, 6))\n",
    "    fig.suptitle('Avg. Predictions all Stocks', fontsize=12)\n",
    "\n",
    "    ax[0, 0].set_title('NN', fontsize=10)\n",
    "    ax[0, 1].set_title('XGBoost', fontsize=10)\n",
    "    ax[1, 0].set_title('Lewellen', fontsize=10)\n",
    "\n",
    "    # loop over unique stocks found in the second level\n",
    "    for stock in stocks:\n",
    "        ax[0, 0].plot(df_models_avg_true_best['NN', stock], 'b', label='Actual')\n",
    "        ax[0, 0].plot(df_models_avg_pre_best['NN', stock], 'r', label='Predicted')\n",
    "\n",
    "        ax[0, 1].plot(df_models_avg_true_best['XGB', stock], 'b', label='Actual')\n",
    "        ax[0, 1].plot(df_models_avg_pre_best['XGB', stock], 'r', label='Predicted')\n",
    "\n",
    "        ax[1, 0].plot(df_benchmarks['lewellen', stock, 'true'], 'b', label='Actual')\n",
    "        ax[1, 0].plot(df_benchmarks['lewellen', stock, 'predictions'], 'r', label='Predicted')\n",
    "\n",
    "    ax[0, 0].set_xticks([ax[0, 0].get_xticks()[0], ax[0, 0].get_xticks()[-1]], minor=False)\n",
    "    ax[0, 0].set_xticklabels([start_time.strftime(\"%b %Y\"), end_time.strftime(\"%b %Y\")])\n",
    "\n",
    "    ax[1, 0].set_xticks([ax[1, 0].get_xticks()[0], ax[1, 0].get_xticks()[-1]], minor=False)\n",
    "    ax[1, 0].set_xticklabels([start_time.strftime(\"%b %Y\"), end_time.strftime(\"%b %Y\")])\n",
    "\n",
    "    ax[0, 1].set_xticks([ax[0, 1].get_xticks()[0], ax[0, 1].get_xticks()[-1]], minor=False)\n",
    "    ax[0, 1].set_xticklabels([start_time.strftime(\"%b %Y\"), end_time.strftime(\"%b %Y\")])\n",
    "\n",
    "    # add fake line to add one legend only\n",
    "    custom_lines = [Line2D([0], [0], color='b', lw=2),\n",
    "                    Line2D([0], [0], color='r', lw=2)]\n",
    "\n",
    "    ax[1, 1].legend(custom_lines, ['Actual', 'Predicted'], loc='center')\n",
    "    ax[1, 1].axis('off')\n",
    "\n",
    "    plt.savefig(os.path.join(save_in, f'{unique_string}_predictions_avg_all_stocks.png'),\n",
    "                orientation='portrait',\n",
    "                format='png')\n",
    "    plt.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Main script\n",
    "- Try 2 seeds and 3 learning rates (i.e. 6 combinations)\n",
    "\n",
    "- For each combination:\n",
    "    - Fit nn model (lstm1, nn1, nn2, nn3, nn4, nn5)    \n",
    "    - Get walk forward validation evaluation metrics\n",
    "    - Make predictions on test data\n",
    "    - Fit XGBoost model\n",
    "    - Get walk forward validation evaluation metrics\n",
    "    - Make predictions on test data\n",
    "    - Make predictions on test data using Lewellen benchmark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stocks_csv_dir_name = os.path.join(os.getcwd(), 'data', 'weekly_features')\n",
    "stocks_csv_files_full_path_list = glob(os.path.join(stocks_csv_dir_name, '*.csv'))\n",
    "\n",
    "features = ['stock_features', 'macro_features', 'momentum_features', 'annual_features', 'tensor_product', 'returns']\n",
    "\n",
    "period = 'weekly'\n",
    "lookback = 12\n",
    "step = 1\n",
    "seeds = [1, 42]\n",
    "\n",
    "learning_rates = [0.001, 0.005, 0.01]\n",
    "\n",
    "save_in = os.path.join(os.getcwd(), 'results', f'{period}_features')\n",
    "\n",
    "# ------------------------------------------------ NN ARGS\n",
    "model_function = nn5\n",
    "model_name = 'nn5'\n",
    "\n",
    "activation = activations.relu\n",
    "kernel_regularizer = regularizers.l1(0.001)\n",
    "\n",
    "batch_size = 32\n",
    "epochs = 100\n",
    "unique_string = f'{model_name}_relu_adam_{epochs}'\n",
    "\n",
    "# ------------------------------------------------ XGB ARGS\n",
    "objective = 'reg:squarederror'\n",
    "max_depth = 2\n",
    "n_estimators = 1000\n",
    "\n",
    "unique_string = f\"{unique_string}_xgb_{max_depth}_{n_estimators}\"\n",
    "\n",
    "# ------------------------------------------------------------------------------------------------------------------\n",
    "# create empty MultiIndexed DataFrames to save all results for all stocks\n",
    "df_models = pd.DataFrame(columns=pd.MultiIndex(levels=[[], [], [], [], []],\n",
    "                                               codes=[[], [], [], [], []],\n",
    "                                               names=['model', 'stock', 'lr', 'seed', 'true_pre']))\n",
    "df_benchmarks = pd.DataFrame(columns=pd.MultiIndex(levels=[[], [], []],\n",
    "                                                   codes=[[], [], []],\n",
    "                                                   names=['benchmark', 'stock', 'true_pre']))\n",
    "df_models_eval_metrics = pd.DataFrame(columns=pd.MultiIndex(levels=[[], [], []],\n",
    "                                                            codes=[[], [], []],\n",
    "                                                            names=['data', 'stock', 'lr']))\n",
    "df_models_avg_best_all_stocks = pd.DataFrame()\n",
    "\n",
    "# loop over each stock\n",
    "# for NN and XGB, loop over each learning rate and seed\n",
    "# for lewellen, no need to loop over learning rates and seeds\n",
    "# train a new model, get predictions, then save results in the master DataFrame\n",
    "for stocks_csv_file_full_path in tqdm(stocks_csv_files_full_path_list):\n",
    "        file_name = os.path.basename(stocks_csv_file_full_path)\n",
    "        file_root, _ = os.path.splitext(file_name)\n",
    "        print(f'\\nProcessing {file_name}...')\n",
    "\n",
    "        # loop over learning rates\n",
    "        for learning_rate in tqdm(learning_rates):\n",
    "            print(f'\\nlearning rate {learning_rate}...')\n",
    "            optimizer = optimizers.adam(lr=learning_rate)\n",
    "\n",
    "            # loop over seeds\n",
    "            for seed in tqdm(seeds):\n",
    "                print(f'\\nseed {seed}...')\n",
    "\n",
    "                # -------------------------------------------- NN MODEL --------------------------------------------\n",
    "                print('\\nFitting NN model...')\n",
    "                # create NN object\n",
    "                stock_nn = StocksNN(stocks_csv_file_full_path,\n",
    "                                    period,\n",
    "                                    lookback,\n",
    "                                    step,\n",
    "                                    features,\n",
    "                                    'excess_return')\n",
    "                stock_nn.build_and_compile_model(model_function,\n",
    "                                                 model_name,\n",
    "                                                 activation,\n",
    "                                                 optimizer,\n",
    "                                                 seed,\n",
    "                                                 kernel_regularizer)\n",
    "\n",
    "                df_models_eval_metrics['NN', f'{file_root}', learning_rate] = stock_nn.walk_forward_validation(\n",
    "                    batch_size, epochs)\n",
    "\n",
    "                df_nn_predictions = stock_nn.predict_on_test_data_nn()\n",
    "                df_models['NN', f'{file_root}', learning_rate, seed, 'true'] = df_nn_predictions['true']\n",
    "                df_models['NN', f'{file_root}', learning_rate, seed, 'predictions'] = df_nn_predictions['predictions']\n",
    "\n",
    "                # -------------------------------------------- XGB MODEL -------------------------------------------\n",
    "                print('\\nFitting XGB model...')\n",
    "                # create XGB object\n",
    "                stock_xgb = StocksXGB(stocks_csv_file_full_path,\n",
    "                                      period,\n",
    "                                      lookback,\n",
    "                                      step,\n",
    "                                      features,\n",
    "                                      'excess_return')\n",
    "                stock_xgb.build_model(objective,\n",
    "                                      max_depth,\n",
    "                                      learning_rate,\n",
    "                                      n_estimators,\n",
    "                                      seed)\n",
    "\n",
    "                df_models_eval_metrics['XGB', f'{file_root}', learning_rate] = stock_xgb.walk_forward_validation()\n",
    "\n",
    "                df_xgb_predictions = stock_xgb.predict_on_test_data()\n",
    "                df_models['XGB', f'{file_root}', learning_rate, seed, 'true'] = df_xgb_predictions['true']\n",
    "                df_models['XGB', f'{file_root}', learning_rate, seed, 'predictions'] = df_xgb_predictions[\n",
    "                    'predictions']\n",
    "\n",
    "        # ------------------------------------------------ LEWELLEN\n",
    "        print('Predicting lewellen...')\n",
    "        df_lewellen_pre = stock_nn.predict_on_test_data_lewellen()\n",
    "        df_benchmarks['lewellen', f'{file_root}', 'true'] = df_lewellen_pre['true']\n",
    "        df_benchmarks['lewellen', f'{file_root}', 'predictions'] = df_lewellen_pre['predictions']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Average predictions across seeds\n",
    "- Choose learning rate with the best evaluation scores on the validation set\n",
    "- Calculate evaluation scores on the test set (using chosen learning rate)\n",
    "- Save as csv files\n",
    "- Plot results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# --------------------------------------------------- BENCHMARK ----------------------------------------------------\n",
    "# save predictions\n",
    "df_benchmarks.to_csv(os.path.join(save_in, f'{unique_string}_benchmarks.csv'))\n",
    "\n",
    "# calculate and save test scores\n",
    "df_benchmarks_scores = df_benchmarks.dropna().groupby(level=[0, 1], axis=1).apply(\n",
    "    lambda s: calc_predictions_avg_test_scores(s.iloc[:, 0], s.iloc[:, 1]))\n",
    "df_benchmarks_scores.to_csv(os.path.join(save_in, f'{unique_string}_benchmarks_scores.csv'))\n",
    "\n",
    "# ----------------------------------------------------- MODELS -----------------------------------------------------\n",
    "# save each stock predictions\n",
    "df_models = df_models.dropna(axis=0)\n",
    "df_models.to_csv(os.path.join(save_in, f'{unique_string}_models.csv'))\n",
    "\n",
    "# average per stock predictions across all seeds\n",
    "df_models_avg = df_models.groupby(level=[0, 1, 2, 4], axis=1).mean()\n",
    "df_models_avg.to_csv(os.path.join(save_in, f'{unique_string}_models_avg.csv'))\n",
    "\n",
    "# calculate test scores per stock averaged predictions\n",
    "df_models_avg_scores = df_models_avg.groupby(level=[0, 1, 2], axis=1).apply(\n",
    "    lambda s: calc_predictions_avg_test_scores(s.iloc[:, 0], s.iloc[:, 1]))\n",
    "df_models_avg_scores.to_csv(os.path.join(save_in, f'{unique_string}_models_avg_scores.csv'))\n",
    "\n",
    "# select best learning rate per stock (minimum averaged 'mse' on validation set)\n",
    "df_models_eval_metrics_best = df_models_eval_metrics[\n",
    "    df_models_eval_metrics.groupby(level=[0, 1], axis=1).idxmin().loc['mse']]\n",
    "df_models_eval_metrics_best.to_csv(os.path.join(save_in, f'{unique_string}_models_eval_metrics_best.csv'))\n",
    "\n",
    "# select metrics for best learning rate per stock (using previous best learning rates on validation)\n",
    "df_models_avg_scores_best = df_models_avg_scores[df_models_eval_metrics_best.columns]\n",
    "df_models_avg_scores_best.to_csv(os.path.join(save_in, f'{unique_string}_models_avg_scores_best.csv'))\n",
    "\n",
    "# select true/pre values for best learning rate per stock (using previous best learning rates on validation)\n",
    "# separate true from predictions to get rid of the forth level\n",
    "df_models_avg_true = df_models_avg.loc[:, (slice(None), slice(None), slice(None), 'true')]\n",
    "df_models_avg_true.columns = df_models_avg_true.columns.droplevel(3)\n",
    "df_models_avg_true_best = df_models_avg_true[df_models_eval_metrics_best.columns]\n",
    "df_models_avg_true_best.to_csv(os.path.join(save_in, f'{unique_string}_models_avg_true_best.csv'))\n",
    "\n",
    "df_models_avg_pre = df_models_avg.loc[:, (slice(None), slice(None), slice(None), 'predictions')]\n",
    "df_models_avg_pre.columns = df_models_avg_pre.columns.droplevel(3)\n",
    "df_models_avg_pre_best = df_models_avg_pre[df_models_eval_metrics_best.columns]\n",
    "df_models_avg_pre_best.to_csv(os.path.join(save_in, f'{unique_string}_models_avg_pre_best.csv'))\n",
    "\n",
    "# plot per stock\n",
    "plot_predictions_avg_stock(df_models_avg_true_best,\n",
    "                           df_models_avg_pre_best,\n",
    "                           df_benchmarks,\n",
    "                           save_in,\n",
    "                           unique_string)\n",
    "# plot all stocks\n",
    "plot_predictions_avg_all_stocks(df_models_avg_true_best,\n",
    "                           df_models_avg_pre_best,\n",
    "                           df_benchmarks,\n",
    "                           save_in,\n",
    "                           unique_string)\n",
    "\n",
    "# concatenate averaged predictions for all stocks vertically\n",
    "# get rid of the third level\n",
    "df_models_avg_true_best.columns = df_models_avg_true_best.columns.droplevel(2)\n",
    "df_models_avg_true_best_all_stocks = df_models_avg_true_best.groupby(level=[0], axis=1).apply(\n",
    "    lambda df: df.values.flatten())\n",
    "\n",
    "df_models_avg_pre_best.columns = df_models_avg_pre_best.columns.droplevel(2)\n",
    "df_models_avg_pre_best_all_stocks = df_models_avg_pre_best.groupby(level=[0], axis=1).apply(\n",
    "    lambda df: df.values.flatten())\n",
    "\n",
    "df_models_avg_best_all_stocks['NN'] = calc_predictions_avg_test_scores(\n",
    "    df_models_avg_true_best_all_stocks.loc['NN'],\n",
    "    df_models_avg_pre_best_all_stocks.loc['NN'])\n",
    "\n",
    "df_models_avg_best_all_stocks['XGB'] = calc_predictions_avg_test_scores(\n",
    "    df_models_avg_true_best_all_stocks.loc['XGB'],\n",
    "    df_models_avg_pre_best_all_stocks.loc['XGB'])\n",
    "\n",
    "df_benchmarks_all_stocks = df_benchmarks.groupby(level=[0, 2], axis=1).apply(\n",
    "    lambda df: df.values.flatten())\n",
    "\n",
    "df_models_avg_best_all_stocks['lewellen'] = calc_predictions_avg_test_scores(\n",
    "    df_benchmarks_all_stocks.loc['lewellen', 'true'],\n",
    "    df_benchmarks_all_stocks.loc['lewellen', 'predictions'])\n",
    "\n",
    "df_models_avg_best_all_stocks.to_csv(os.path.join(save_in, f'{unique_string}_models_avg_best_all_stocks.csv'))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
