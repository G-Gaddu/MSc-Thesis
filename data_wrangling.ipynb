{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1- Features Engineering"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Daily features"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Define a function that takes an Excel file and extract daily features. The function returns a dictionary of 17 DataFrames. Each DataFrame includes the values of one feature for every single Stock."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_daily_predictors(excel_file_path):\n",
    "    \n",
    "    # First obtain the betas\n",
    "    df_beta = pd.read_excel(excel_file_path, \"Raw Beta Hard\", index_col=0, parse_dates=True).apply(\n",
    "        lambda x: x.fillna(x.median()), axis=1)\n",
    "\n",
    "    # Then the beta squareds\n",
    "    df_beta_sq = pd.read_excel(excel_file_path, \"Raw Beta Squared Hard\", index_col=0, parse_dates=True).apply(\n",
    "        lambda x: x.fillna(x.median()), axis=1)\n",
    "\n",
    "    # Then the book to market\n",
    "    df_market_to_book = pd.read_excel(excel_file_path, \"Market to book hard\", index_col=0, parse_dates=True).apply(\n",
    "        lambda x: x.fillna(x.median()), axis=1)\n",
    "    df_book_to_market = df_market_to_book.rdiv(1)\n",
    "\n",
    "    # Then the volume\n",
    "    df_volume = pd.read_excel(excel_file_path, \"Volume hard\", index_col=0, parse_dates=True).apply(\n",
    "        lambda x: x.fillna(x.median()), axis=1)\n",
    "\n",
    "    # Then the turnover\n",
    "    df_turn_over = pd.read_excel(excel_file_path, \"Turnover hard\", index_col=0, parse_dates=True).apply(\n",
    "        lambda x: x.fillna(x.median()), axis=1)\n",
    "\n",
    "    # Then the individual volatilities\n",
    "    df_volatility = pd.read_excel(excel_file_path, \"10,20,30,60,90 day vol hard\", index_col=0,\n",
    "                                  header=[0, 1], parse_dates=True)\n",
    "\n",
    "    df_vol_10_day = df_volatility.loc[:, (slice(None), 'Volatility 10 Day')].apply(\n",
    "        lambda x: x.fillna(x.median()), axis=1)\n",
    "    df_vol_10_day.columns = df_vol_10_day.columns.droplevel(1)\n",
    "\n",
    "    df_vol_20_day = df_volatility.loc[:, (slice(None), 'Volatility 20 Day')].apply(\n",
    "        lambda x: x.fillna(x.median()), axis=1)\n",
    "    df_vol_20_day.columns = df_vol_20_day.columns.droplevel(1)\n",
    "\n",
    "    df_vol_30_day = df_volatility.loc[:, (slice(None), 'Volatility 30 Day')].apply(\n",
    "        lambda x: x.fillna(x.median()), axis=1)\n",
    "    df_vol_30_day.columns = df_vol_30_day.columns.droplevel(1)\n",
    "\n",
    "    df_vol_60_day = df_volatility.loc[:, (slice(None), 'Volatility 60 Day')].apply(\n",
    "        lambda x: x.fillna(x.median()), axis=1)\n",
    "    df_vol_60_day.columns = df_vol_60_day.columns.droplevel(1)\n",
    "\n",
    "    df_vol_90_day = df_volatility.loc[:, (slice(None), 'Volatility 90 Day')].apply(\n",
    "        lambda x: x.fillna(x.median()), axis=1)\n",
    "    df_vol_90_day.columns = df_vol_90_day.columns.droplevel(1)\n",
    "\n",
    "    # Then get monthly bid and ask prices as well as the spread\n",
    "    df_bid_ask = pd.read_excel(excel_file_path, \"Bid Ask Hard\", index_col=0, header=[0, 1], parse_dates=True)\n",
    "\n",
    "    df_bid = df_bid_ask.loc[:, (slice(None), 'Bid Price')].apply(\n",
    "        lambda x: x.fillna(x.median()), axis=1)\n",
    "    df_bid.columns = df_bid.columns.droplevel(1)\n",
    "\n",
    "    df_ask = df_bid_ask.loc[:, (slice(None), 'Ask Price')].apply(\n",
    "        lambda x: x.fillna(x.median()), axis=1)\n",
    "    df_ask.columns = df_ask.columns.droplevel(1)\n",
    "\n",
    "    df_bid_ask_spread = df_bid.sub(df_ask)\n",
    "\n",
    "    # Then the RSI\n",
    "    rsi = pd.read_excel(excel_file_path, \"RSI 3,9,14,30 day hard\", index_col=0, header=[0, 1], parse_dates=True)\n",
    "\n",
    "    df_rsi_3_days = rsi.loc[:, (slice(None), 'RSI 3 Day')].apply(\n",
    "        lambda x: x.fillna(x.median()), axis=1)\n",
    "    df_rsi_3_days.columns = df_rsi_3_days.columns.droplevel(1)\n",
    "\n",
    "    df_rsi_9_days = rsi.loc[:, (slice(None), 'RSI 9 Day')].apply(\n",
    "        lambda x: x.fillna(x.median()), axis=1)\n",
    "    df_rsi_9_days.columns = df_rsi_9_days.columns.droplevel(1)\n",
    "\n",
    "    df_rsi_14_days = rsi.loc[:, (slice(None), 'RSI 14 Day')].apply(\n",
    "        lambda x: x.fillna(x.median()), axis=1)\n",
    "    df_rsi_14_days.columns = df_rsi_14_days.columns.droplevel(1)\n",
    "\n",
    "    df_rsi_30_days = rsi.loc[:, (slice(None), 'RSI 30 Day')].apply(\n",
    "        lambda x: x.fillna(x.median()), axis=1)\n",
    "    df_rsi_30_days.columns = df_rsi_30_days.columns.droplevel(1)\n",
    "\n",
    "    return {'df_beta': df_beta,\n",
    "            'df_beta_sq': df_beta_sq,\n",
    "            'df_book_to_market': df_book_to_market,\n",
    "            'df_volume': df_volume,\n",
    "            'df_turn_over': df_turn_over,\n",
    "            'df_vol_10_day': df_vol_10_day,\n",
    "            'df_vol_20_day': df_vol_20_day,\n",
    "            'df_vol_30_day': df_vol_30_day,\n",
    "            'df_vol_60_day': df_vol_60_day,\n",
    "            'df_vol_90_day': df_vol_90_day,\n",
    "            'df_bid': df_bid,\n",
    "            'df_ask': df_ask,\n",
    "            'df_bid_ask_spread': df_bid_ask_spread,\n",
    "            'df_rsi_3_days': df_rsi_3_days,\n",
    "            'df_rsi_9_days': df_rsi_9_days,\n",
    "            'df_rsi_14_days': df_rsi_14_days,\n",
    "            'df_rsi_30_days': df_rsi_30_days}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Weekly features"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Define a function that takes an Excel file and extract weekly features. The function returns a dictionary of 17 DataFrames. Each DataFrame includes the values of one feature for every single Stock."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_weekly_predictors(excel_file_path):\n",
    "    \n",
    "    # First obtain the betas\n",
    "    df_beta = pd.read_excel(excel_file_path, \"Raw Beta Hard weekly\", index_col=0, parse_dates=True).resample(\n",
    "        'W').last().apply(lambda x: x.fillna(x.median()), axis=1)\n",
    "\n",
    "    # Then the beta squareds\n",
    "    df_beta_sq = pd.read_excel(excel_file_path, \"Raw beta squared weekly hard\", index_col=0,\n",
    "                               parse_dates=True).resample('W').last().apply(lambda x: x.fillna(x.median()), axis=1)\n",
    "\n",
    "    # Then the book to market\n",
    "    df_market_to_book = pd.read_excel(excel_file_path, \"Market to book hard weekly\", index_col=0,\n",
    "                                      parse_dates=True).resample('W').last().apply(lambda x: x.fillna(x.median()),\n",
    "                                                                                   axis=1)\n",
    "    df_book_to_market = df_market_to_book.rdiv(1)\n",
    "\n",
    "    # Then the volume\n",
    "    df_volume = pd.read_excel(excel_file_path, \"Volume hard weekly\", index_col=0,\n",
    "                              parse_dates=True).resample('W').last().apply(lambda x: x.fillna(x.median()), axis=1)\n",
    "\n",
    "    # Then the turnover\n",
    "    df_turn_over = pd.read_excel(excel_file_path, \"Turnover hard weekly\", index_col=0,\n",
    "                                 parse_dates=True).resample('W').last().apply(lambda x: x.fillna(x.median()), axis=1)\n",
    "\n",
    "    # Then the individual volatilities\n",
    "    df_volatility = pd.read_excel(excel_file_path, \"10,20,30,60,90 day vol hard\", index_col=0,\n",
    "                                  header=[0, 1], parse_dates=True)\n",
    "\n",
    "    df_vol_10_day = df_volatility.loc[:, (slice(None), 'Volatility 10 Day')].resample('W').last().apply(\n",
    "        lambda x: x.fillna(x.median()), axis=1)\n",
    "    df_vol_10_day.columns = df_vol_10_day.columns.droplevel(1)\n",
    "\n",
    "    df_vol_20_day = df_volatility.loc[:, (slice(None), 'Volatility 20 Day')].resample('W').last().apply(\n",
    "        lambda x: x.fillna(x.median()), axis=1)\n",
    "    df_vol_20_day.columns = df_vol_20_day.columns.droplevel(1)\n",
    "\n",
    "    df_vol_30_day = df_volatility.loc[:, (slice(None), 'Volatility 30 Day')].resample('W').last().apply(\n",
    "        lambda x: x.fillna(x.median()), axis=1)\n",
    "    df_vol_30_day.columns = df_vol_30_day.columns.droplevel(1)\n",
    "\n",
    "    df_vol_60_day = df_volatility.loc[:, (slice(None), 'Volatility 60 Day')].resample('W').last().apply(\n",
    "        lambda x: x.fillna(x.median()), axis=1)\n",
    "    df_vol_60_day.columns = df_vol_60_day.columns.droplevel(1)\n",
    "\n",
    "    df_vol_90_day = df_volatility.loc[:, (slice(None), 'Volatility 90 Day')].resample('W').last().apply(\n",
    "        lambda x: x.fillna(x.median()), axis=1)\n",
    "    df_vol_90_day.columns = df_vol_90_day.columns.droplevel(1)\n",
    "\n",
    "    # Then get monthly bid and ask prices as well as the spread\n",
    "    df_bid_ask = pd.read_excel(excel_file_path, \"Bid ask hard weekly\", index_col=0, header=[0, 1], parse_dates=True)\n",
    "\n",
    "    df_bid = df_bid_ask.loc[:, (slice(None), 'Bid Price')].resample('W').last().apply(\n",
    "        lambda x: x.fillna(x.median()), axis=1)\n",
    "    df_bid.columns = df_bid.columns.droplevel(1)\n",
    "\n",
    "    df_ask = df_bid_ask.loc[:, (slice(None), 'Ask Price')].resample('W').last().apply(\n",
    "        lambda x: x.fillna(x.median()), axis=1)\n",
    "    df_ask.columns = df_ask.columns.droplevel(1)\n",
    "\n",
    "    df_bid_ask_spread = df_bid.sub(df_ask)\n",
    "\n",
    "    # Then the RSI\n",
    "    rsi = pd.read_excel(excel_file_path, \"RSI 3,9,14,30 week hard\", index_col=0, header=[0, 1], parse_dates=True)\n",
    "\n",
    "    df_rsi_3_days = rsi.loc[:, (slice(None), 'RSI 3 Day')].resample('W').last().apply(\n",
    "        lambda x: x.fillna(x.median()), axis=1)\n",
    "    df_rsi_3_days.columns = df_rsi_3_days.columns.droplevel(1)\n",
    "\n",
    "    df_rsi_9_days = rsi.loc[:, (slice(None), 'RSI 9 Day')].resample('W').last().apply(\n",
    "        lambda x: x.fillna(x.median()), axis=1)\n",
    "    df_rsi_9_days.columns = df_rsi_9_days.columns.droplevel(1)\n",
    "\n",
    "    df_rsi_14_days = rsi.loc[:, (slice(None), 'RSI 14 Day')].resample('W').last().apply(\n",
    "        lambda x: x.fillna(x.median()), axis=1)\n",
    "    df_rsi_14_days.columns = df_rsi_14_days.columns.droplevel(1)\n",
    "\n",
    "    df_rsi_30_days = rsi.loc[:, (slice(None), 'RSI 30 Day')].resample('W').last().apply(\n",
    "        lambda x: x.fillna(x.median()), axis=1)\n",
    "    df_rsi_30_days.columns = df_rsi_30_days.columns.droplevel(1)\n",
    "\n",
    "    return {'df_beta': df_beta,\n",
    "            'df_beta_sq': df_beta_sq,\n",
    "            'df_book_to_market': df_book_to_market,\n",
    "            'df_volume': df_volume,\n",
    "            'df_turn_over': df_turn_over,\n",
    "            'df_vol_10_day': df_vol_10_day,\n",
    "            'df_vol_20_day': df_vol_20_day,\n",
    "            'df_vol_30_day': df_vol_30_day,\n",
    "            'df_vol_60_day': df_vol_60_day,\n",
    "            'df_vol_90_day': df_vol_90_day,\n",
    "            'df_bid': df_bid,\n",
    "            'df_ask': df_ask,\n",
    "            'df_bid_ask_spread': df_bid_ask_spread,\n",
    "            'df_rsi_3_days': df_rsi_3_days,\n",
    "            'df_rsi_9_days': df_rsi_9_days,\n",
    "            'df_rsi_14_days': df_rsi_14_days,\n",
    "            'df_rsi_30_days': df_rsi_30_days}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Monthly features"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-  Define a function that takes an Excel file and extract monthly features. The function returns a dictionary of 17 DataFrames. Each DataFrame includes the values of one feature for every single Stock."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_monthly_predictors(excel_file_path):\n",
    "    \n",
    "    # First obtain the betas\n",
    "    df_beta = pd.read_excel(excel_file_path, \"Raw Beta Hard\", index_col=0, parse_dates=True).resample(\n",
    "        'M').last().apply(lambda x: x.fillna(x.median()), axis=1)\n",
    "\n",
    "    # Then the beta squareds\n",
    "    df_beta_sq = pd.read_excel(excel_file_path, \"Raw Beta Squared Hard\", index_col=0, parse_dates=True).resample(\n",
    "        'M').last().apply(lambda x: x.fillna(x.median()), axis=1)\n",
    "\n",
    "    # Then the book to market\n",
    "    df_market_to_book = pd.read_excel(excel_file_path, \"Market to book hard\", index_col=0, parse_dates=True).resample(\n",
    "        'M').last().apply(lambda x: x.fillna(x.median()), axis=1)\n",
    "    df_book_to_market = df_market_to_book.rdiv(1)\n",
    "\n",
    "    # Then the volume\n",
    "    df_volume = pd.read_excel(excel_file_path, \"Volume hard\", index_col=0, parse_dates=True).resample(\n",
    "        'M').last().apply(lambda x: x.fillna(x.median()), axis=1)\n",
    "\n",
    "    # Then the turnover\n",
    "    df_turn_over = pd.read_excel(excel_file_path, \"Turnover hard\", index_col=0, parse_dates=True).resample(\n",
    "        'M').last().apply(lambda x: x.fillna(x.median()), axis=1)\n",
    "\n",
    "    # Then the individual volatilities\n",
    "    df_volatility = pd.read_excel(excel_file_path, \"10,20,30,60,90 day vol hard\", index_col=0, header=[0, 1], parse_dates=True)\n",
    "\n",
    "    df_vol_10_day = df_volatility.loc[:, (slice(None), 'Volatility 10 Day')].resample('M').last().apply(\n",
    "        lambda x: x.fillna(x.median()), axis=1)\n",
    "    df_vol_10_day.columns = df_vol_10_day.columns.droplevel(1)\n",
    "\n",
    "    df_vol_20_day = df_volatility.loc[:, (slice(None), 'Volatility 20 Day')].resample('M').last().apply(\n",
    "        lambda x: x.fillna(x.median()), axis=1)\n",
    "    df_vol_20_day.columns = df_vol_20_day.columns.droplevel(1)\n",
    "\n",
    "    df_vol_30_day = df_volatility.loc[:, (slice(None), 'Volatility 30 Day')].resample('M').last().apply(\n",
    "        lambda x: x.fillna(x.median()), axis=1)\n",
    "    df_vol_30_day.columns = df_vol_30_day.columns.droplevel(1)\n",
    "\n",
    "    df_vol_60_day = df_volatility.loc[:, (slice(None), 'Volatility 60 Day')].resample('M').last().apply(\n",
    "        lambda x: x.fillna(x.median()), axis=1)\n",
    "    df_vol_60_day.columns = df_vol_60_day.columns.droplevel(1)\n",
    "\n",
    "    df_vol_90_day = df_volatility.loc[:, (slice(None), 'Volatility 90 Day')].resample('M').last().apply(\n",
    "        lambda x: x.fillna(x.median()), axis=1)\n",
    "    df_vol_90_day.columns = df_vol_90_day.columns.droplevel(1)\n",
    "\n",
    "    # Then get monthly bid and ask prices as well as the spread\n",
    "    df_bid_ask = pd.read_excel(excel_file_path, \"Bid Ask Hard\", index_col=0, header=[0, 1], parse_dates=True)\n",
    "\n",
    "    df_bid = df_bid_ask.loc[:, (slice(None), 'Bid Price')].resample('M').last().apply(\n",
    "        lambda x: x.fillna(x.median()), axis=1)\n",
    "    df_bid.columns = df_bid.columns.droplevel(1)\n",
    "\n",
    "    df_ask = df_bid_ask.loc[:, (slice(None), 'Ask Price')].resample('M').last().apply(\n",
    "        lambda x: x.fillna(x.median()), axis=1)\n",
    "    df_ask.columns = df_ask.columns.droplevel(1)\n",
    "\n",
    "    df_bid_ask_spread = df_bid.sub(df_ask)\n",
    "\n",
    "    # Then the RSI\n",
    "    rsi = pd.read_excel(excel_file_path, \"RSI 3,9,14 and 30 day hard\", index_col=0, header=[0, 1], parse_dates=True)\n",
    "\n",
    "    df_rsi_3_days = rsi.loc[:, (slice(None), 'RSI 3 Day')].resample('M').last().apply(\n",
    "        lambda x: x.fillna(x.median()), axis=1)\n",
    "    df_rsi_3_days.columns = df_rsi_3_days.columns.droplevel(1)\n",
    "\n",
    "    df_rsi_9_days = rsi.loc[:, (slice(None), 'RSI 9 Day')].resample('M').last().apply(\n",
    "        lambda x: x.fillna(x.median()), axis=1)\n",
    "    df_rsi_9_days.columns = df_rsi_9_days.columns.droplevel(1)\n",
    "\n",
    "    df_rsi_14_days = rsi.loc[:, (slice(None), 'RSI 14 Day')].resample('M').last().apply(\n",
    "        lambda x: x.fillna(x.median()), axis=1)\n",
    "    df_rsi_14_days.columns = df_rsi_14_days.columns.droplevel(1)\n",
    "\n",
    "    df_rsi_30_days = rsi.loc[:, (slice(None), 'RSI 30 Day')].resample('M').last().apply(\n",
    "        lambda x: x.fillna(x.median()), axis=1)\n",
    "    df_rsi_30_days.columns = df_rsi_30_days.columns.droplevel(1)\n",
    "\n",
    "    return {'df_beta': df_beta,\n",
    "            'df_beta_sq': df_beta_sq,\n",
    "            'df_book_to_market': df_book_to_market,\n",
    "            'df_volume': df_volume,\n",
    "            'df_turn_over': df_turn_over,\n",
    "            'df_vol_10_day': df_vol_10_day,\n",
    "            'df_vol_20_day': df_vol_20_day,\n",
    "            'df_vol_30_day': df_vol_30_day,\n",
    "            'df_vol_60_day': df_vol_60_day,\n",
    "            'df_vol_90_day': df_vol_90_day,\n",
    "            'df_bid': df_bid,\n",
    "            'df_ask': df_ask,\n",
    "            'df_bid_ask_spread': df_bid_ask_spread,\n",
    "            'df_rsi_3_days': df_rsi_3_days,\n",
    "            'df_rsi_9_days': df_rsi_9_days,\n",
    "            'df_rsi_14_days': df_rsi_14_days,\n",
    "            'df_rsi_30_days': df_rsi_30_days}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Annual features"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Define a function that takes an Excel file and extract annual features. The function returns a dictionary of 24 DataFrames. Each DataFrame includes the values of one feature for every single Stock."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_annual_predictors(excel_file_path):\n",
    "\n",
    "    # First get the cash to debt figures for each company for every year. First load the tab\n",
    "    # Then fill in NA values with the cross sectional median as per Gu Kelly and Xiu\n",
    "    df_cash_to_debt = pd.read_excel(excel_file_path, \"Cash flow to debt hard\", index_col=0,\n",
    "                                    parse_dates=True).resample('Y').last().apply(lambda x: x.fillna(x.median()),\n",
    "                                                                                 axis=1).fillna(method='ffill')\n",
    "\n",
    "\n",
    "    #  The load the cash productivity tab and treat the free cash flow and cash and cash holdings figures.\n",
    "    df_cash_prod = pd.read_excel(excel_file_path, \"Cash Productivity Hard\", index_col=0,\n",
    "                                 header=[0, 1], parse_dates=True)\n",
    "\n",
    "    df_fcf = df_cash_prod.loc[:, (slice(None), 'Free Cash Flow')].resample('Y').last().apply(\n",
    "        lambda x: x.fillna(x.median()), axis=1).fillna(method='ffill')\n",
    "    df_fcf.columns = df_fcf.columns.droplevel(1)\n",
    "\n",
    "    df_cash = df_cash_prod.loc[:, (slice(None), 'Cash and Cash Equivalents')].resample('Y').last().apply(\n",
    "        lambda x: x.fillna(x.median()), axis=1).fillna(method='ffill')\n",
    "    df_cash.columns = df_cash.columns.droplevel(1)\n",
    "\n",
    "    df_cash_prod = df_fcf.div(df_cash)\n",
    "\n",
    "    # Then load the cash flow to price tab\n",
    "    df_price_to_cash_flow = pd.read_excel(excel_file_path, \"Price to Cash Flow Hard\", index_col=0,\n",
    "                                          parse_dates=True).resample('Y').last().apply(lambda x: x.fillna(x.median()),\n",
    "                                                                                       axis=1).fillna(method='ffill')\n",
    "    df_cash_flow_to_price = df_price_to_cash_flow.rdiv(1)\n",
    "\n",
    "    # Then the change in outstanding shares\n",
    "    df_no_shares = pd.read_excel(excel_file_path, \"Change in shares outstanding ha\", index_col=0,\n",
    "                                 parse_dates=True).resample('Y').last().apply(lambda x: x.fillna(x.median()),\n",
    "                                                                              axis=1).fillna(method='ffill')\n",
    "    df_change_in_shares = df_no_shares - df_no_shares.shift(1)\n",
    "    df_change_in_shares = df_change_in_shares.drop(df_change_in_shares.index[0])\n",
    "\n",
    "    # Then the current ratio\n",
    "    df_current_ratio = pd.read_excel(excel_file_path, \"Current Ratio Hard\", index_col=0,\n",
    "                                     parse_dates=True).resample('Y').last().apply(lambda x: x.fillna(x.median()),\n",
    "                                                                                  axis=1).fillna(method='ffill')\n",
    "\n",
    "    # Then the dividend yield\n",
    "    df_div_yield = pd.read_excel(excel_file_path, \"Dividend Yield Hard\", index_col=0,\n",
    "                                 parse_dates=True).resample('Y').last().apply(\n",
    "        lambda x: x.fillna(x.median()), axis=1).fillna(method='ffill')\n",
    "\n",
    "    # Then the annual common equity growth\n",
    "    df_tot_eq = pd.read_excel(excel_file_path, \"Annual common equity growth ha\", index_col=0,\n",
    "                              parse_dates=True).resample('Y').last().apply(lambda x: x.fillna(x.median()),\n",
    "                                                                           axis=1).fillna(method='ffill')\n",
    "    df_an_eq_growth = df_tot_eq - df_tot_eq.shift(1)\n",
    "    df_an_eq_growth = df_an_eq_growth.drop(df_an_eq_growth.index[0])\n",
    "\n",
    "    # Then the price to earnings ratio\n",
    "    df_price_ear = pd.read_excel(excel_file_path, \"Price to Earnings Ratio Hard\", index_col=0,\n",
    "                                 parse_dates=True).resample('Y').last().apply(lambda x: x.fillna(x.median()),\n",
    "                                                                              axis=1).fillna(method='ffill')\n",
    "\n",
    "    # Then the gross profitability\n",
    "    df_gross_pro = pd.read_excel(excel_file_path, \"Gross Profitability Hard\", index_col=0,\n",
    "                                 header=[0, 1], parse_dates=True)\n",
    "\n",
    "    df_gross_profit = df_gross_pro.loc[:, (slice(None), 'Gross Profit')].resample('Y').last().apply(\n",
    "        lambda x: x.fillna(x.median()), axis=1).fillna(method='ffill')\n",
    "    df_gross_profit.columns = df_gross_profit.columns.droplevel(1)\n",
    "\n",
    "    df_total_assets = df_gross_pro.loc[:, (slice(None), 'Total Assets')].resample('Y').last().apply(\n",
    "        lambda x: x.fillna(x.median()), axis=1).fillna(method='ffill')\n",
    "    df_total_assets.columns = df_total_assets.columns.droplevel(1)\n",
    "\n",
    "    df_gros_pro_rat = df_gross_profit.div(df_total_assets)\n",
    "\n",
    "    # Then the 1 year growth in capex\n",
    "    df_cap_ex_growth = pd.read_excel(excel_file_path, \"Growth in capital exp hard\", index_col=0,\n",
    "                                     parse_dates=True).resample('Y').last().apply(lambda x: x.fillna(x.median()),\n",
    "                                                                                  axis=1).fillna(method='ffill')\n",
    "\n",
    "    # Then the 1 year employee growth\n",
    "    df_employee_growth = pd.read_excel(excel_file_path, \"Employee Growth hard\", index_col=0,\n",
    "                                       parse_dates=True).resample('Y').last().apply(lambda x: x.fillna(x.median()),\n",
    "                                                                                    axis=1).fillna(method='ffill')\n",
    "\n",
    "    # Then for capital expenditures and inventories\n",
    "    df_cap_ex_inve = pd.read_excel(excel_file_path, \"Capital Expenditures and inv ha\", index_col=0,\n",
    "                                   header=[0, 1], parse_dates=True)\n",
    "\n",
    "    df_cap_ex = df_cap_ex_inve.loc[:, (slice(None), 'Capital Expenditures')].resample(\n",
    "        'Y').last().apply(lambda x: x.fillna(x.median()), axis=1).fillna(method='ffill')\n",
    "    df_cap_ex.columns = df_cap_ex.columns.droplevel(1)\n",
    "\n",
    "    df_inventories = df_cap_ex_inve.loc[:, (slice(None), 'Inventories')].resample(\n",
    "        'Y').last().apply(lambda x: x.fillna(x.median()), axis=1).fillna(method='ffill')\n",
    "    df_inventories.columns = df_inventories.columns.droplevel(1)\n",
    "\n",
    "    df_cap_inv = df_cap_ex.add(df_inventories)\n",
    "\n",
    "    # Then financial leverage\n",
    "    df_leverage = pd.read_excel(excel_file_path, \"Leverage Hard\", index_col=0, parse_dates=True).resample(\n",
    "        'Y').last().apply(lambda x: x.fillna(x.median()), axis=1).fillna(method='ffill')\n",
    "\n",
    "    # Then quick ratio\n",
    "    df_quick_ratio = pd.read_excel(excel_file_path, \"Quick Ratio hard\", index_col=0, parse_dates=True).resample(\n",
    "        'Y').last().apply(lambda x: x.fillna(x.median()), axis=1).fillna(method='ffill')\n",
    "\n",
    "    # Then total capital\n",
    "    df_total_capital = pd.read_excel(excel_file_path, \"Total Capital hard\", index_col=0, parse_dates=True).resample(\n",
    "        'Y').last().apply(lambda x: x.fillna(x.median()), axis=1).fillna(method='ffill')\n",
    "\n",
    "    # Then the return on assets\n",
    "    df_roa = pd.read_excel(excel_file_path, \"Return on Assets hard\", index_col=0, parse_dates=True).resample(\n",
    "        'Y').last().apply(lambda x: x.fillna(x.median()), axis=1).fillna(method='ffill')\n",
    "\n",
    "    # Then the return on equity\n",
    "    df_roe = pd.read_excel(excel_file_path, \"Return on Equity hard\", index_col=0, parse_dates=True).resample(\n",
    "        'Y').last().apply(lambda x: x.fillna(x.median()), axis=1).fillna(method='ffill')\n",
    "\n",
    "    # Then the return on invested capital\n",
    "    df_roi = pd.read_excel(excel_file_path, \"Return on invested capital hard\", index_col=0, parse_dates=True).resample(\n",
    "        'Y').last().apply(lambda x: x.fillna(x.median()), axis=1).fillna(method='ffill')\n",
    "\n",
    "    # Then the sales to inventory ratio\n",
    "    df_sales_to_inv = pd.read_excel(excel_file_path, \"Sales to inventories hard\", index_col=0,\n",
    "                                    parse_dates=True).resample('Y').last().apply(lambda x: x.fillna(x.median()),\n",
    "                                                                                 axis=1).fillna(method='ffill')\n",
    "\n",
    "    # Then the sales to accounts receivables\n",
    "    df_sales_to_acc = pd.read_excel(excel_file_path, \"Sales to accounts receivables h\", index_col=0,\n",
    "                                    parse_dates=True).resample('Y').last().apply(lambda x: x.fillna(x.median()),\n",
    "                                                                                 axis=1).fillna(method='ffill')\n",
    "\n",
    "    # Then the sales to price\n",
    "    df_price_to_sales = pd.read_excel(excel_file_path, \"Price to sales hard\", index_col=0,\n",
    "                                      parse_dates=True).resample('Y').last().apply(lambda x: x.fillna(x.median()),\n",
    "                                                                                   axis=1).fillna(method='ffill')\n",
    "    df_sales_to_price = df_price_to_sales.rdiv(1)\n",
    "\n",
    "    # Then the sales growth\n",
    "    df_sales_grow = pd.read_excel(excel_file_path, \"Sales Growth Hard\", index_col=0, parse_dates=True).resample(\n",
    "        'Y').last().apply(lambda x: x.fillna(x.median()), axis=1).fillna(method='ffill')\n",
    "\n",
    "    # Then sales to cash\n",
    "    df_sales_cash = pd.read_excel(excel_file_path, \"Sales to Cash hard\", index_col=0, parse_dates=True).resample(\n",
    "        'Y').last().apply(lambda x: x.fillna(x.median()), axis=1).fillna(method='ffill')\n",
    "\n",
    "    # Then the sales to inventory % change\n",
    "    df_sales_inv_per = pd.read_excel(excel_file_path, \"Sales to inventories hard %\", index_col=0,\n",
    "                                     parse_dates=True).resample('Y').last().apply(lambda x: x.fillna(x.median()),\n",
    "                                                                                  axis=1).fillna(method='ffill')\n",
    "\n",
    "    df_sales_inv_perc = ((df_sales_inv_per.div(df_sales_inv_per.shift(1))) - 1) * 100\n",
    "    df_sales_inv_perc = df_sales_inv_perc.drop(df_sales_inv_perc.index[0])\n",
    "\n",
    "    return {'df_cash_to_debt': df_cash_to_debt,\n",
    "            'df_cash_prod': df_cash_prod,\n",
    "            'df_cash_flow_to_price': df_cash_flow_to_price,\n",
    "            'df_change_in_shares': df_change_in_shares,\n",
    "            'df_current_ratio': df_current_ratio,\n",
    "            'df_div_yield': df_div_yield,\n",
    "            'df_an_eq_growth': df_an_eq_growth,\n",
    "            'df_price_ear': df_price_ear,\n",
    "            'df_gros_pro_rat': df_gros_pro_rat,\n",
    "            'df_employee_growth': df_employee_growth,\n",
    "            'df_cap_ex_growth': df_cap_ex_growth,\n",
    "            'df_cap_inv': df_cap_inv,\n",
    "            'df_leverage': df_leverage,\n",
    "            'df_quick_ratio': df_quick_ratio,\n",
    "            'df_total_capital': df_total_capital,\n",
    "            'df_roa': df_roa,\n",
    "            'df_roe': df_roe,\n",
    "            'df_roi': df_roi,\n",
    "            'df_sales_to_inv': df_sales_to_inv,\n",
    "            'df_sales_to_acc': df_sales_to_acc,\n",
    "            'df_sales_to_price': df_sales_to_price,\n",
    "            'df_sales_grow': df_sales_grow,\n",
    "            'df_sales_cash': df_sales_cash,\n",
    "            'df_sales_inv_perc': df_sales_inv_perc}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Macro features"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Define a function that takes an Excel file and extract Macro features. The function returns one DataFrame, which will be used for all stocks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_macro_predictors(excel_file_path, period):    \n",
    "\n",
    "    sheet = None\n",
    "    resample_by = None\n",
    "\n",
    "    if period == 'daily':\n",
    "        sheet = 'Eurostoxx data daily hard'\n",
    "        resample_by = 'D'\n",
    "    elif period == 'weekly':\n",
    "        sheet = 'Eurostoxx data weekly hard'\n",
    "        resample_by = 'W'\n",
    "    elif period == 'monthly':\n",
    "        sheet = 'Eurostoxx data monthly hard'\n",
    "        resample_by = 'M'\n",
    "\n",
    "    df_macro = pd.read_excel(excel_file_path, sheet, index_col=0, parse_dates=True).resample(\n",
    "        resample_by).last().drop(columns=['BEst Div Yld']).apply(lambda x: x.fillna(x.interpolate()),\n",
    "                                                                 axis=1)\n",
    "\n",
    "    # Read in the price to book ratio and then invert it\n",
    "    df_macro['Price to Book Ratio'] = df_macro['Price to Book Ratio'].rdiv(1)\n",
    "\n",
    "    # Then the price to earnings ratio\n",
    "    df_macro['Price Earnings Ratio (P/E)'] = df_macro['Price Earnings Ratio (P/E)'].rdiv(1)\n",
    "\n",
    "    return df_macro"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Euribor features"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Define a function that takes an Excel file and extract Euribor features. The function returns one DataFrame, which will be used for all stocks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_euribor_rates(excel_file_path, period):\n",
    "\n",
    "    sheet = None\n",
    "    resample_by = None\n",
    "    drop_col = None\n",
    "\n",
    "    if period == 'weekly':\n",
    "        sheet = '1 week Euribor Hard'\n",
    "        resample_by = 'W'\n",
    "        drop_col = ['EUR001W Index']\n",
    "    elif period == 'monthly':\n",
    "        sheet = '1 month Euribor Hard'\n",
    "        resample_by = 'M'\n",
    "        drop_col = ['EUR001M Index']\n",
    "\n",
    "    df_euribo = pd.read_excel(excel_file_path, sheet, index_col=0, parse_dates=True).resample(\n",
    "        resample_by).last().drop(columns=drop_col).apply(lambda x: x.fillna(x.interpolate()),\n",
    "                                                                  axis=1)\n",
    "\n",
    "    return df_euribo"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Additional Macro features"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Define a function that takes an Excel file and extract Additional Macro features. The function returns one DataFrame, which will be used for all stocks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_daily_additional_macro_predictors(excel_file_path):\n",
    "    \n",
    "    # Dataframe of Spot prices. Remove all zero values\n",
    "    df_spot = pd.read_excel(excel_file_path, \"FX SPOT HARD\", parse_dates=True, index_col=0)\n",
    "    df_spot = df_spot[(df_spot != 0).all(axis=1)]\n",
    "\n",
    "    # Calculating spot returns to be further used in calculating 2M realized volatilities\n",
    "    # TODO: by default the result is written back to the edge of the window but we can make it at the center\n",
    "    # TODO: why shifting by 1?\n",
    "    df_returns = df_spot.pct_change().dropna()\n",
    "    df_realized_vol_2m = (df_returns.rolling(window=22 * 2).std() * np.sqrt(252)).shift(1).dropna()\n",
    "    df_realized_vol_2m.columns = [col + ' Vol2M' for col in df_realized_vol_2m.columns]\n",
    "\n",
    "    # Calculating 1W change in realized Volatilities\n",
    "    # TODO: the shift changed from 3 to 5\n",
    "    df_1w_vol_per_change = (df_realized_vol_2m / df_realized_vol_2m.shift(5) - 1).dropna()\n",
    "    df_1w_vol_per_change.columns = [col + ' 1W' for col in df_1w_vol_per_change.columns]\n",
    "\n",
    "    # Calculating 1month change in realized Volatilities\n",
    "    df_1m_vol_per_change = (df_realized_vol_2m / df_realized_vol_2m.shift(22) - 1).dropna()\n",
    "    df_1m_vol_per_change.columns = [col + ' 1M' for col in df_1m_vol_per_change.columns]\n",
    "\n",
    "    # join Volatilite, 1W change in vols and 1M change in realized vols\n",
    "    df_main = df_realized_vol_2m.join(df_1w_vol_per_change).join(df_1m_vol_per_change).dropna()\n",
    "\n",
    "    for sheet in [\"ATM VOLS HARD\", \"3M 25D RR HARD\"]:\n",
    "        df = pd.read_excel(excel_file_path, sheet, parse_dates=True, index_col=0).dropna(axis=1)\n",
    "        df = df[(df != 0).all(axis=1)]\n",
    "        df_main = df_main.join(df.shift(1)).dropna()\n",
    "\n",
    "    # looping through sheets to calculate 1week and 1month change\n",
    "    # and joining them with df_main\n",
    "    for sheet in [\"FX SPOT HARD\", \"ATM VOLS HARD\", \"3M 25D RR HARD\", \"3M DEPOSIT RATES HARD\", \"10Y YIELD HARD\",\n",
    "                  \"EQUITY INDICES HARD\", \"COMDTY HARD\", \"CREDIT SPREADS HARD\", \"IMM POSITIONING HARD\"]:\n",
    "        df = pd.read_excel(excel_file_path, sheet, parse_dates=True, index_col=0).dropna(axis=1)\n",
    "        df = df[(df != 0).all(axis=1)]\n",
    "\n",
    "        df_1w_per_change = (df / df.shift(5) - 1).dropna()\n",
    "        df_1w_per_change.columns = [col + ' 1W' for col in df_1w_per_change.columns]\n",
    "        df_main = df_main.join(df_1w_per_change.shift(1)).dropna()\n",
    "\n",
    "        df_1m_per_change = (df / df.shift(22) - 1).dropna()\n",
    "        df_1m_per_change.columns = [col + ' 1M' for col in df_1m_per_change.columns]\n",
    "        df_main = df_main.join(df_1m_per_change.shift(1)).dropna()\n",
    "\n",
    "    # Remove all zero values\n",
    "    df_easi = pd.read_excel(excel_file_path, \"JPM EASI HARD\", parse_dates=True, index_col=0).dropna(axis=1)\n",
    "    df_easi = df_easi[(df_easi != 0).all(axis=1)]\n",
    "\n",
    "    # JPM EASI is an index value between -100 to +100, so we have divided by total\n",
    "    # range (200) to find out change in 1W and 1M\n",
    "    df_easi_1w = ((df_easi - df_easi.shift(5)) / 200).dropna()\n",
    "    df_easi_1w.columns = [col + ' 1W' for col in df_easi_1w.columns]\n",
    "    df_main = df_main.join(df_easi_1w.shift(1)).dropna()\n",
    "\n",
    "    df_easi_1m = ((df_easi - df_easi.shift(22)) / 200).dropna()\n",
    "    df_easi_1m.columns = [col + ' 1M' for col in df_easi_1m.columns]\n",
    "    df_main = df_main.join(df_easi_1m.shift(1)).dropna()\n",
    "\n",
    "    df_main.to_csv(os.path.join(os.getcwd(), 'data', 'Daily_Additional_Macro_Processed.csv'))\n",
    "\n",
    "    return df_main"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2- Save Engineered Features Per Stock"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-  Define a function that creates one CSV file per stock. The CSV file includes all engineered features (and the target), for a certain period of time (daily, monthly, annually)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_csv_per_stock(df_returns,\n",
    "                       df_excess_return,\n",
    "                       df_monthly_momentum,\n",
    "                       df_6_months_momentum,\n",
    "                       df_12_months_momentum,\n",
    "                       features_df_dict,\n",
    "                       df_macro_features,\n",
    "                       df_additional_macro_features,\n",
    "                       annual_features_df_dict,\n",
    "                       tensor_product,\n",
    "                       period):\n",
    "    \"\"\"\n",
    "    Join all given features for a given period for all stocks\n",
    "    Save each stock results in a csv file\n",
    "    \"\"\"\n",
    "    # loop over stocks\n",
    "    for stock in df_returns.columns:\n",
    "        print(f'Processing stock {stock}...')\n",
    "\n",
    "        df = pd.DataFrame(columns=pd.MultiIndex(levels=[[], []],\n",
    "                                                codes=[[], []],\n",
    "                                                names=['data', 'features']))\n",
    "\n",
    "        save_in_sub_dir = f'{period}_features'\n",
    "\n",
    "        # loop over dict items {'predictor name': predictor_DataFrame}\n",
    "        for k, v in features_df_dict.items():\n",
    "            df['stock_features', k] = v[stock]\n",
    "\n",
    "        # inner join with macro\n",
    "        if df_macro_features is not None:\n",
    "            # make a copy and add a column level so we can join\n",
    "            df_macro_features_temp = df_macro_features.copy()\n",
    "            df_macro_features_temp.columns = pd.MultiIndex.from_product([['macro_features'],\n",
    "                                                                         df_macro_features_temp.columns])\n",
    "            df = df.join(df_macro_features_temp).apply(lambda x: x.fillna(x.median()), axis=0)\n",
    "\n",
    "        if df_monthly_momentum is not None:\n",
    "            # make a copy and add a column level so we can join\n",
    "            df_monthly_momentum_temp = df_monthly_momentum.copy()\n",
    "            df_monthly_momentum_temp[stock].name = ('momentum_features', 'momentum_1M')\n",
    "            df = df.join(df_monthly_momentum_temp[stock], how='inner')\n",
    "\n",
    "        if df_6_months_momentum is not None:\n",
    "            # make a copy and add a column level so we can join\n",
    "            df_6_months_momentum_temp = df_6_months_momentum.copy()\n",
    "            df_6_months_momentum_temp[stock].name = ('momentum_features', 'momentum_6M')\n",
    "            df = df.join(df_6_months_momentum_temp[stock], how='inner')\n",
    "\n",
    "        if df_12_months_momentum is not None:\n",
    "            # make a copy and add a column level so we can join\n",
    "            df_12_months_momentum_temp = df_12_months_momentum.copy()\n",
    "            df_12_months_momentum_temp[stock].name = ('momentum_features', 'momentum_12M')\n",
    "            df = df.join(df_12_months_momentum_temp[stock], how='inner')\n",
    "\n",
    "        # inner join with additional macro\n",
    "        if df_additional_macro_features is not None:\n",
    "            # make a copy and add a column level so we can join\n",
    "            df_additional_macro_features_temp = df_additional_macro_features.copy()\n",
    "            df_additional_macro_features_temp.columns = pd.MultiIndex.from_product([['additional_macro_features'],\n",
    "                                                                                    df_additional_macro_features_temp.columns])\n",
    "            df = df.join(df_additional_macro_features_temp).apply(lambda x: x.fillna(x.median()), axis=0)\n",
    "\n",
    "        # augment with annual features if given\n",
    "        # loop over dict items {'predictor name': predictor_DataFrame}\n",
    "        if annual_features_df_dict is not None:\n",
    "            df_annual = pd.DataFrame()\n",
    "\n",
    "            for k, v in annual_features_df_dict.items():\n",
    "                df_annual[k] = v[stock]\n",
    "\n",
    "            # add a column level so we can join\n",
    "            df_annual.columns = pd.MultiIndex.from_product([['annual_features'], df_annual.columns])\n",
    "\n",
    "            # re-sampling annual to daily/weekly results in NAs for all days/weeks but the last\n",
    "            # inner join\n",
    "            if period == 'daily':\n",
    "                df = df.join(df_annual.resample('D').last().fillna(method='ffill')).dropna()\n",
    "            elif period == 'weekly':\n",
    "                df = df.join(df_annual.resample('W').last().fillna(method='ffill')).dropna()\n",
    "            elif period == 'monthly':\n",
    "                df = df.join(df_annual.resample('M').last().fillna(method='ffill')).dropna()\n",
    "\n",
    "        if tensor_product:\n",
    "            # returns a series of lists\n",
    "            s_tensor_product = df.apply(lambda s: np.kron(s[['stock_features', 'momentum_features', 'annual_features']],\n",
    "                                                          s[['macro_features']]), axis=1)\n",
    "            # convert series of lists to df\n",
    "            df_tensor_product = pd.DataFrame.from_dict(dict(zip(s_tensor_product.index, s_tensor_product.values))).T\n",
    "\n",
    "            # add a column level so we can join\n",
    "            df_tensor_product.columns = pd.MultiIndex.from_product([['tensor_product'], df_tensor_product.columns])\n",
    "            df = df.join(df_tensor_product).apply(lambda x: x.fillna(x.median()), axis=0)\n",
    "\n",
    "        # make a copy and add a column level so we can join\n",
    "        df_returns_temp = df_returns.copy()\n",
    "        df_returns_temp[stock].name = ('returns', 'return')\n",
    "        df = df.join(df_returns_temp[stock]).apply(lambda x: x.fillna(x.median()), axis=0)\n",
    "\n",
    "        if df_excess_return is not None:\n",
    "            # make a copy and add a column level so we can join\n",
    "            df_excess_return_temp = df_excess_return.copy()\n",
    "            df_excess_return_temp[stock].name = ('returns', 'excess_return')\n",
    "            df = df.join(df_excess_return_temp[stock]).dropna(axis=0)\n",
    "\n",
    "        # make sure there's a sub directory to save results to\n",
    "        save_in = os.path.join(os.getcwd(), 'data', save_in_sub_dir)\n",
    "        if not os.path.exists(save_in):\n",
    "            os.makedirs(save_in)\n",
    "\n",
    "        df.to_csv(os.path.join(save_in, f'{stock}.csv'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3- Data Wrangling"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### All Excel files paths"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "macro_features_file_path = os.path.join(os.getcwd(), 'data', r'Macro_Features.xlsx')\n",
    "daily_features_file_path = os.path.join(os.getcwd(), 'data', 'Daily_Features.xlsx')\n",
    "daily_additional_macro_features_file_path = os.path.join(os.getcwd(), 'data', r'Daily_Additional_Macro.xlsx')\n",
    "daily_prices_file_path = os.path.join(os.getcwd(), 'data', 'Daily_Prices.csv')\n",
    "weekly_prices_file_path = os.path.join(os.getcwd(), 'data', 'Weekly_Prices.csv')\n",
    "weekly_euribor_file_path = os.path.join(os.getcwd(), 'data', 'Euribor_Rates.xlsx')\n",
    "weekly_features_file_path = os.path.join(os.getcwd(), 'data', 'Weekly_Features.xlsx')\n",
    "monthly_prices_file_path = os.path.join(os.getcwd(), 'data', 'Monthly_Prices.csv')\n",
    "monthly_euribor_file_path = os.path.join(os.getcwd(), 'data', 'Euribor_Rates.xlsx')\n",
    "monthly_features_file_path = os.path.join(os.getcwd(), 'data', 'Monthly_Features.xlsx')\n",
    "annual_features_file_path = os.path.join(os.getcwd(), 'data', 'Annual_Features.xlsx')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Annual features for all Stocks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_dict_annual_features = get_annual_predictors(annual_features_file_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Daily returns and features for each Stock as CSV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_daily_prices = pd.read_csv(daily_prices_file_path, index_col=0, parse_dates=True, dayfirst=True).dropna(\n",
    "    axis='columns')\n",
    "\n",
    "df_daily_returns = df_daily_prices.apply(lambda s: s - s.shift(1)).dropna()\n",
    "df_daily_momentum_1m = df_daily_returns.apply(lambda s: s / s.shift(22) - 1).dropna()\n",
    "df_daily_momentum_6m = df_daily_returns.apply(lambda s: s / s.shift(132) - 1).dropna()\n",
    "df_daily_momentum_12m = df_daily_returns.apply(lambda s: s / s.shift(264) - 1).dropna()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_dict_daily_features = get_daily_predictors(daily_features_file_path)\n",
    "df_daily_macro_features = get_macro_predictors(macro_features_file_path, 'daily')\n",
    "df_daily_additional_macro_features = get_daily_additional_macro_predictors(daily_additional_macro_features_file_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "save_csv_per_stock(df_daily_returns,\n",
    "                   None,\n",
    "                   df_daily_momentum_1m,\n",
    "                   df_daily_momentum_6m,\n",
    "                   df_daily_momentum_12m,\n",
    "                   df_dict_daily_features,\n",
    "                   df_daily_macro_features,\n",
    "                   df_daily_additional_macro_features,\n",
    "                   df_dict_annual_features,\n",
    "                   tensor_product=True,\n",
    "                   period='daily')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Weekly returns and features for each Stock as CSV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_weekly_prices = pd.read_csv(weekly_prices_file_path, index_col=0, parse_dates=True).dropna(\n",
    "    axis='columns').resample('W').last()\n",
    "\n",
    "df_weekly_returns = df_weekly_prices.apply(lambda s: s - s.shift(1)).dropna()\n",
    "df_weekly_momentum_1m = df_weekly_prices.apply(lambda s: s / s.shift(4) - 1).dropna()\n",
    "df_weekly_momentum_6m = df_weekly_prices.apply(lambda s: s / s.shift(24) - 1).dropna()\n",
    "df_weekly_momentum_12m = df_weekly_prices.apply(lambda s: s / s.shift(48) - 1).dropna()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_weekly_euribor = get_euribor_rates(weekly_euribor_file_path, period='weekly')\n",
    "df_weekly_excess_return = df_weekly_returns.subtract(df_weekly_euribor.iloc[:, 0] / 100, axis=0)\n",
    "df_dict_weekly_features = get_weekly_predictors(weekly_features_file_path)\n",
    "df_weekly_macro_features = get_macro_predictors(macro_features_file_path, 'weekly')\n",
    "df_weekly_additional_macro_features = df_daily_additional_macro_features.resample('W').last().dropna()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "save_csv_per_stock(df_weekly_returns,\n",
    "                   df_weekly_excess_return,\n",
    "                   df_weekly_momentum_1m,\n",
    "                   df_weekly_momentum_6m,\n",
    "                   df_weekly_momentum_12m,\n",
    "                   df_dict_weekly_features,\n",
    "                   df_weekly_macro_features,\n",
    "                   df_weekly_additional_macro_features,\n",
    "                   df_dict_annual_features,\n",
    "                   tensor_product=True,\n",
    "                   period='weekly')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Monthly returns and features for each Stock as CSV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_monthly_prices = pd.read_csv(monthly_prices_file_path, index_col=0, parse_dates=True).dropna(\n",
    "    axis='columns').resample('M').last()\n",
    "df_monthly_returns = df_monthly_prices.apply(lambda s: s - s.shift(1)).dropna()\n",
    "df_monthly_momentum_1m = df_monthly_prices.apply(lambda s: s / s.shift(1) - 1).dropna()\n",
    "df_monthly_momentum_6m = df_monthly_prices.apply(lambda s: s / s.shift(6) - 1).dropna()\n",
    "df_monthly_momentum_12m = df_monthly_prices.apply(lambda s: s / s.shift(12) - 1).dropna()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_monthly_euribor = get_euribor_rates(monthly_euribor_file_path, period='monthly')\n",
    "df_monthly_excess_return = df_monthly_returns.subtract(df_monthly_euribor.iloc[:, 0] / 100, axis=0)\n",
    "df_dict_monthly_features = get_monthly_predictors(monthly_features_file_path)\n",
    "df_monthly_macro_features = get_macro_predictors(macro_features_file_path, 'monthly')\n",
    "df_monthly_additional_macro_features = df_daily_additional_macro_features.resample('M').last().dropna()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "save_csv_per_stock(df_monthly_returns,\n",
    "                   df_monthly_excess_return,\n",
    "                   df_monthly_momentum_1m,\n",
    "                   df_monthly_momentum_6m,\n",
    "                   df_monthly_momentum_12m,\n",
    "                   df_dict_monthly_features,\n",
    "                   df_monthly_macro_features,\n",
    "                   df_monthly_additional_macro_features,\n",
    "                   df_dict_annual_features,\n",
    "                   tensor_product=True,\n",
    "                   period='monthly')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
